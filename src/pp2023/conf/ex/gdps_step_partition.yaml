## Partition the dataset on the step dimension, to train MLPs separately for each 
# step.


defaults:
- dataset: gdps_prebatch_partition
- model: mlp
- distribution: quantile
- optimizer: adam
- scheduler: one_cycle
- _self_

timeout_min: 1200

batch_size:  # Use prebatched dataset.
max_epochs: 2000 
log_every_n_steps: 100
early_stopping_patience: 800

variable_idx: 

model:
  use_step_embedding: false
  use_step_feature: false

scheduler:
  instance:
    max_lr: 1e-3
    pct_start: 0.05
    final_div_factor: 100
